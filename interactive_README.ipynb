{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Effective TensorFlow\n",
    "\n",
    "Table of Contents\n",
    "=================\n",
    "1.  [TensorFlow Basics](#basics)\n",
    "2.  [Understanding static and dynamic shapes](#shapes)\n",
    "3.  [Scopes and when to use them](#scopes)\n",
    "4.  [Broadcasting the good and the ugly](#broadcast)\n",
    "5.  [Feeding data to TensorFlow](#data)\n",
    "6.  [Take advantage of the overloaded operators](#overloaded_ops)\n",
    "7.  [Understanding order of execution and control dependencies](#control_deps)\n",
    "8.  [Control flow operations: conditionals and loops](#control_flow)\n",
    "9.  [Prototyping kernels and advanced visualization with Python ops](#python_ops)\n",
    "10. [Multi-GPU processing with data parallelism](#multi_gpu)\n",
    "11. [Debugging TensorFlow models](#debug)\n",
    "12. [Numerical stability in TensorFlow](#stable)\n",
    "13. [Building a neural network training framework with learn API](#tf_learn)\n",
    "14. [TensorFlow Cookbook](#cookbook)\n",
    "    - [Get shape](#get_shape)\n",
    "    - [Batch gather](#batch_gather)\n",
    "    - [Beam search](#beam_search)\n",
    "    - [Merge](#merge)\n",
    "    - [Entropy](#entropy)\n",
    "    - [KL-Divergence](#kld)\n",
    "    - [Make parallel](#make_parallel)\n",
    "    - [Leaky Relu](#leaky_relu)\n",
    "    - [Batch normalization](#batch_norm)\n",
    "    - [Squeeze and excitation](#squeeze_excite)\n",
    "---\n",
    "\n",
    "_We aim to gradually expand this series by adding new articles and keep the content up to date with the latest releases of TensorFlow API. If you have suggestions on how to improve this series or find the explanations ambiguous, feel free to create an issue, send patches, or reach out by email._\n",
    "\n",
    "You can instantiate a notebook with the following examples in your browser using Binder (click the badge):\n",
    "[![Binder](https://mybinder.org/badge.svg)](https://mybinder.org/v2/gh/MTDzi/EffectiveTensorflow/master?filepath=interactive_README.ipynb)\n",
    "or create an environment:\n",
    "```\n",
    "conda env create -f environment.yml\n",
    "```\n",
    "and run the `\"interactive_README.ipynb\"` notebook on your local machine.\n",
    "\n",
    " _We encourage you to also check out the accompanied neural network training framework built on top of tf.contrib.learn API. The [framework](https://github.com/vahidk/TensorflowFramework) can be downloaded separately:_\n",
    "```\n",
    "git clone https://github.com/vahidk/TensorflowFramework.git\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TensorFlow Basics\n",
    "<a name=\"basics\"></a>\n",
    "\n",
    "----\n",
    "\n",
    "The most striking difference between TensorFlow and other numerical computation libraries such as NumPy is that operations in TensorFlow are symbolic. This is a powerful concept that allows TensorFlow to do all sort of things (e.g. automatic differentiation) that are not possible with imperative libraries such as NumPy. But it also comes at the cost of making it harder to grasp. Our attempt here is to demystify TensorFlow and provide some guidelines and best practices for more effective use of TensorFlow.\n",
    "\n",
    "Let's start with a simple example, we want to multiply two random matrices. First we look at an implementation done in NumPy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.7485095  -5.59024267  3.93082445  2.55917956  0.59277674 -6.66552939\n",
      "  -2.40458467  5.4319916   5.56142796 -0.8046387 ]\n",
      " [ 1.96875406  0.65974864  0.08796587  2.1907204   2.00727243 -2.30403248\n",
      "  -2.71932968  4.63759934  3.27892142  2.32663759]\n",
      " [-3.15577612 -0.22104057  1.9102264   0.4812299  10.16662424 -5.06317832\n",
      "   5.44408346  3.76546059  2.26798929  0.66407198]\n",
      " [ 1.15711978 -3.61599519  3.68453927  5.40623152  1.58809846 -4.37684757\n",
      "   5.28756711 -0.02459983  0.45859242 -4.4702995 ]\n",
      " [ 0.65622415 -2.41083612  3.62594699  3.50381739  1.5898958  -6.0850136\n",
      "   0.2537588   3.62316909  2.28379474 -2.53507167]\n",
      " [ 2.98777607 -2.49173551 -5.23638723 -0.84965334  4.66772438  0.1322222\n",
      "  -2.2116159   3.0213242   3.51217043  5.80251129]\n",
      " [-4.05476462  1.28011728  0.12540454 -4.02218679 -2.98586911  3.30389793\n",
      "   1.08564238 -2.86037779  2.59396895  2.70202705]\n",
      " [-1.22809578  3.38440873 -4.85645672  0.55186651  6.66086302  2.01685461\n",
      "   1.88975582 -0.64930878  0.53819056  5.35426998]\n",
      " [ 2.38030901 -4.71833143  2.62392265  1.03392227 -1.47163682 -5.48045119\n",
      "  -1.19753175  3.45659077  0.69684322 -5.39723743]\n",
      " [-0.50564702  4.91961849 -2.26479397 -3.16512343 -2.3987083   3.85778185\n",
      "   1.13677466 -1.31230863 -3.38453363 -2.25819655]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "x = np.random.normal(size=[10, 10])\n",
    "y = np.random.normal(size=[10, 10])\n",
    "z = np.dot(x, y)\n",
    "\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we perform the exact same computation this time in TensorFlow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.9984558   1.2202401   4.5406218  -5.6154556   3.7718744  -1.076636\n",
      "  -0.33144772  0.36278403  1.7673626  -1.8776131 ]\n",
      " [ 1.3766351   2.2370672  -4.5856566  -0.29603451  6.9201193   0.33479798\n",
      "  -0.25443816  2.26128    -1.082665   -2.204284  ]\n",
      " [-2.0409524   0.7671233  -0.97869277  1.1314733   0.83344823 -0.90907055\n",
      "  -0.40174296  3.0174084   1.4677465   2.530023  ]\n",
      " [-1.3911192   2.3426912  -3.546411    5.0523553  -4.0902333   0.7310281\n",
      "  -1.2013087  -0.93665236  1.9893584  -1.1253839 ]\n",
      " [-1.3854306   1.2025392  -0.5575047   2.2230158   3.4019444   1.187032\n",
      "   1.1981499   1.753017    4.0591335  -0.6848878 ]\n",
      " [ 0.42980057  3.5802028  -1.450702   -0.8094818  -0.847203   -0.75621456\n",
      "  -0.04248752 -1.5006583   2.4389415   1.0657905 ]\n",
      " [-2.3185875   3.0392463   1.8161279   0.6634381  -2.700938   -0.07138385\n",
      "   1.8476132  -3.715016    1.6490449  -2.1208625 ]\n",
      " [-5.322461    4.7942586  -3.2232206   3.1059303  -3.1260638  -1.1886452\n",
      "   0.9416484  -1.1749064  -1.54753    -0.31409425]\n",
      " [-5.442473   -0.3711123   1.8529902   1.197406   -8.735816   -1.5236738\n",
      "  -4.351056   -2.236982   -7.809832   -7.9936213 ]\n",
      " [-1.519416   -1.7387117   2.5269942   1.3061132   2.489622    0.08219633\n",
      "   2.514927    2.3646734   1.3358974   2.3757067 ]]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "x = tf.random_normal([10, 10])\n",
    "y = tf.random_normal([10, 10])\n",
    "z = tf.matmul(x, y)\n",
    "\n",
    "sess = tf.Session()\n",
    "z_val = sess.run(z)\n",
    "\n",
    "print(z_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unlike NumPy that immediately performs the computation and produces the result, tensorflow only gives us a handle (of type Tensor) to a node in the graph that represents the result. If we try printing the value of z directly, we get something like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'MatMul:0' shape=(10, 10) dtype=float32>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since both the inputs have a fully defined shape, tensorflow is able to infer the shape of the tensor as well as its type. In order to compute the value of the tensor we need to create a session and evaluate it using Session.run() method.\n",
    "\n",
    "\n",
    "***\n",
    "__Tip__: When using Jupyter notebook make sure to call tf.reset_default_graph() at the beginning to clear the symbolic graph before defining new nodes.\n",
    "***\n",
    "\n",
    "To understand how powerful symbolic computation can be let's have a look at another example. Assume that we have samples from a curve (say f(x) = 5x^2 + 3) and we want to estimate f(x) based on these samples. We define a parametric function g(x, w) = w0 x^2 + w1 x + w2, which is a function of the input x and latent parameters w, our goal is then to find the latent parameters such that g(x, w) ≈ f(x). This can be done by minimizing the following loss function: L(w) = &sum; (f(x) - g(x, w))^2. Although there's a closed form solution for this simple problem, we opt to use a more general approach that can be applied to any arbitrary differentiable function, and that is using stochastic gradient descent. We simply compute the average gradient of L(w) with respect to w over a set of sample points and move in the opposite direction.\n",
    "\n",
    "\n",
    "Here's how it can be done in TensorFlow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[4.9936991e+00],\n",
      "       [5.5755395e-04],\n",
      "       [3.3681273e+00]], dtype=float32)]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7efd8995f198>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "# Placeholders are used to feed values from python to TensorFlow ops. We define\n",
    "# two placeholders, one for input feature x, and one for output y.\n",
    "x = tf.placeholder(tf.float32)\n",
    "y = tf.placeholder(tf.float32)\n",
    "\n",
    "# Assuming we know that the desired function is a polynomial of 2nd degree, we\n",
    "# allocate a vector of size 3 to hold the coefficients. The variable will be\n",
    "# automatically initialized with random noise.\n",
    "w = tf.get_variable(\"w\", shape=[3, 1])\n",
    "\n",
    "# We define yhat to be our estimate of y.\n",
    "f = tf.stack([tf.square(x), x, tf.ones_like(x)], 1)\n",
    "yhat = tf.squeeze(tf.matmul(f, w), 1)\n",
    "\n",
    "# The loss is defined to be the l2 distance between our estimate of y and its\n",
    "# true value. We also added a shrinkage term, to ensure the resulting weights\n",
    "# would be small.\n",
    "loss = tf.nn.l2_loss(yhat - y) + 0.1 * tf.nn.l2_loss(w)\n",
    "\n",
    "# We use the Adam optimizer with learning rate set to 0.1 to minimize the loss.\n",
    "train_op = tf.train.AdamOptimizer(0.1).minimize(loss)\n",
    "\n",
    "def generate_data():\n",
    "    x_val = np.random.uniform(-10.0, 10.0, size=100)\n",
    "    y_val = 5 * np.square(x_val) + 3\n",
    "    return x_val, y_val\n",
    "\n",
    "sess = tf.Session()\n",
    "\n",
    "losses_val = []\n",
    "# Since we are using variables we first need to initialize them.\n",
    "sess.run(tf.global_variables_initializer())\n",
    "for _ in range(1000):\n",
    "    x_val, y_val = generate_data()\n",
    "    _, loss_val = sess.run([train_op, loss], {x: x_val, y: y_val})\n",
    "    losses_val.append(loss_val)\n",
    "    \n",
    "plt.plot(losses_val)\n",
    "plt.title('Loss on the validation set')\n",
    "plt.xlabel('Step')\n",
    "plt.ylabel('Loss')\n",
    "\n",
    "print(sess.run([w]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By running this piece of code you should see a result close to this:\n",
    "```\n",
    "[4.9924135, 0.00040895029, 3.4504161]\n",
    "```\n",
    "Which is a relatively close approximation to our parameters.\n",
    "\n",
    "This is just tip of the iceberg for what TensorFlow can do. Many problems such as optimizing large neural networks with millions of parameters can be implemented efficiently in TensorFlow in just a few lines of code. TensorFlow takes care of scaling across multiple devices, and threads, and supports a variety of platforms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding static and dynamic shapes\n",
    "<a name=\"shapes\"></a>\n",
    "Tensors in TensorFlow have a static shape attribute which is determined during graph construction. The static shape may be underspecified. For example we might define a tensor of shape [None, 128]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a = tf.placeholder(tf.float32, [None, 128])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This means that the first dimension can be of any size and will be determined dynamically during Session.run(). You can query the static shape of a Tensor as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[None, 128]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.shape.as_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get the dynamic shape of the tensor you can call tf.shape op, which returns a tensor representing the shape of the given tensor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Shape:0' shape=(2,) dtype=int32>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.shape(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The static shape of a tensor can be set with Tensor.set_shape() method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a.set_shape([32, 128])  # static shape of a is [32, 128]\n",
    "a.set_shape([None, 128])  # first dimension of a is determined dynamically"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can reshape a given tensor dynamically using tf.reshape function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Reshape:0' shape=(32, 128) dtype=float32>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a =  tf.reshape(a, [32, 128])\n",
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can be convenient to have a function that returns the static shape when available and dynamic shape when it's not. The following utility function does just that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_shape(tensor):\n",
    "    static_shape = tensor.shape.as_list()\n",
    "    dynamic_shape = tf.unstack(tf.shape(tensor))\n",
    "    dims = [s[1] if s[0] is None else s[0]\n",
    "            for s in zip(static_shape, dynamic_shape)]\n",
    "    return dims"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now imagine we want to convert a Tensor of rank 3 to a tensor of rank 2 by collapsing the second and third dimensions into one. We can use our get_shape() function to do that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Reshape_1:0' shape=(?, 320) dtype=float32>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = tf.placeholder(tf.float32, [None, 10, 32])\n",
    "shape = get_shape(b)\n",
    "b = tf.reshape(b, [shape[0], shape[1] * shape[2]])\n",
    "b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that this works whether the shapes are statically specified or not.\n",
    "\n",
    "In fact we can write a general purpose reshape function to collapse any list of dimensions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def reshape(tensor, dims_list):\n",
    "    shape = get_shape(tensor)\n",
    "    dims_prod = []\n",
    "    for dims in dims_list:\n",
    "        if isinstance(dims, int):\n",
    "            dims_prod.append(shape[dims])\n",
    "        elif all([isinstance(shape[d], int) for d in dims]):\n",
    "            dims_prod.append(np.prod([shape[d] for d in dims]))\n",
    "        else:\n",
    "            dims_prod.append(tf.prod([shape[d] for d in dims]))\n",
    "    tensor = tf.reshape(tensor, dims_prod)\n",
    "    return tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then collapsing the second dimension becomes very easy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Reshape_2:0' shape=(?, 320) dtype=float32>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = tf.placeholder(tf.float32, [None, 10, 32])\n",
    "b = reshape(b, [0, [1, 2]])\n",
    "b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scopes and when to use them\n",
    "<a name=\"scopes\"></a>\n",
    "\n",
    "Variables and tensors in TensorFlow have a name attribute that is used to identify them in the symbolic graph. If you don't specify a name when creating a variable or a tensor, TensorFlow automatically assigns a name for you:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Const:0\n",
      "Variable:0\n"
     ]
    }
   ],
   "source": [
    "a = tf.constant(1)\n",
    "print(a.name)\n",
    "\n",
    "b = tf.Variable(1)\n",
    "print(b.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can overwrite the default name by explicitly specifying it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a:0\n",
      "b:0\n"
     ]
    }
   ],
   "source": [
    "a = tf.constant(1, name=\"a\")\n",
    "print(a.name)\n",
    "\n",
    "b = tf.Variable(1, name=\"b\")\n",
    "print(b.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TensorFlow introduces two different context managers to alter the name of tensors and variables. The first is tf.name_scope:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scope/a:0\n",
      "scope/b:0\n",
      "c:0\n"
     ]
    }
   ],
   "source": [
    "with tf.name_scope(\"scope\"):\n",
    "    a = tf.constant(1, name=\"a\")\n",
    "    print(a.name)\n",
    "\n",
    "    b = tf.Variable(1, name=\"b\")\n",
    "    print(b.name)\n",
    "\n",
    "    c = tf.get_variable(name=\"c\", shape=[])\n",
    "    print(c.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that there are two ways to define new variables in TensorFlow, by creating a tf.Variable object or by calling tf.get_variable. Calling tf.get_variable with a new name results in creating a new variable, but if a variable with the same name exists it will raise a ValueError exception, telling us that re-declaring a variable is not allowed.\n",
    "\n",
    "tf.name_scope affects the name of tensors and variables created with tf.Variable, but doesn't impact the variables created with tf.get_variable.\n",
    "\n",
    "Unlike tf.name_scope, tf.variable_scope modifies the name of variables created with tf.get_variable as well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scope_1/a:0\n",
      "scope_1/b:0\n",
      "scope/c:0\n"
     ]
    }
   ],
   "source": [
    "with tf.variable_scope(\"scope\"):\n",
    "    a = tf.constant(1, name=\"a\")\n",
    "    print(a.name)\n",
    "\n",
    "    b = tf.Variable(1, name=\"b\")\n",
    "    print(b.name)\n",
    "\n",
    "    c = tf.get_variable(name=\"c\", shape=[])\n",
    "    print(c.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.variable_scope(\"scope\"):\n",
    "    a1 = tf.get_variable(name=\"a\", shape=[])\n",
    "    # Disallowed\n",
    "    # a2 = tf.get_variable(name=\"a\", shape=[])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But what if we actually want to reuse a previously declared variable? Variable scopes also provide the functionality to do that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We've defined a1 above, so in order to create it again, we need to reset\n",
    "# the default graph\n",
    "tf.reset_default_graph()\n",
    "\n",
    "with tf.variable_scope(\"scope\"):\n",
    "    a1 = tf.get_variable(name=\"a\", shape=[])\n",
    "    \n",
    "with tf.variable_scope(\"scope\", reuse=True):\n",
    "    a2 = tf.get_variable(name=\"a\", shape=[])  # OK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This becomes handy for example when using built-in neural network layers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "image1 = tf.placeholder(tf.float32, shape=[None, 100, 100, 3])\n",
    "image2 = tf.placeholder(tf.float32, shape=[None, 100, 100, 3])\n",
    "\n",
    "\n",
    "with tf.variable_scope('my_scope'):\n",
    "    features1 = tf.layers.conv2d(image1, filters=32, kernel_size=3)\n",
    "    \n",
    "# Use the same convolution weights to process the second image:\n",
    "with tf.variable_scope('my_scope', reuse=True):\n",
    "    features2 = tf.layers.conv2d(image2, filters=32, kernel_size=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This syntax may not look very clean to some. Especially if you want to do lots of variable sharing keeping track of when to define new variables and when to reuse them can be cumbersome and error prone. TensorFlow templates are designed to handle this automatically:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "conv3x32 = tf.make_template(\"conv3x32\", lambda x: tf.layers.conv2d(x, 32, 3))\n",
    "features1 = conv3x32(image1)\n",
    "features2 = conv3x32(image2)  # Will reuse the convolution weights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can turn any function to a TensorFlow template. Upon the first call to a template, the variables defined inside the function would be declared and in the consecutive invocations they would automatically get reused."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Broadcasting the good and the ugly\n",
    "<a name=\"broadcast\"></a>\n",
    "TensorFlow supports broadcasting elementwise operations. Normally when you want to perform operations like addition and multiplication, you need to make sure that shapes of the operands match, e.g. you can’t add a tensor of shape [3, 2] to a tensor of shape [3, 4]. But there’s a special case and that’s when you have a singular dimension. TensorFlow implicitly tiles the tensor across its singular dimensions to match the shape of the other operand. So it’s valid to add a tensor of shape [3, 2] to a tensor of shape [3, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a = tf.constant([[1., 2.], [3., 4.]])\n",
    "b = tf.constant([[1.], [2.]])\n",
    "# c = a + tf.tile(b, [1, 2])\n",
    "c = a + b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Broadcasting allows us to perform implicit tiling which makes the code shorter, and more memory efficient, since we don’t need to store the result of the tiling operation. One neat place that this can be used is when combining features of varying length. In order to concatenate features of varying length we commonly tile the input tensors, concatenate the result and apply some nonlinearity. This is a common pattern across a variety of neural network architectures:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a = tf.random_uniform([5, 3, 5])\n",
    "b = tf.random_uniform([5, 1, 6])\n",
    "\n",
    "# concat a and b and apply nonlinearity\n",
    "tiled_b = tf.tile(b, [1, 3, 1])\n",
    "c = tf.concat([a, tiled_b], 2)\n",
    "d = tf.layers.dense(c, 10, activation=tf.nn.relu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But this can be done more efficiently with broadcasting. We use the fact that f(m(x + y)) is equal to f(mx + my). So we can do the linear operations separately and use broadcasting to do implicit concatenation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pa = tf.layers.dense(a, 10, activation=None)\n",
    "pb = tf.layers.dense(b, 10, activation=None)\n",
    "d = tf.nn.relu(pa + pb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In fact this piece of code is pretty general and can be applied to tensors of arbitrary shape as long as broadcasting between tensors is possible:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def merge(a, b, units, activation=tf.nn.relu):\n",
    "    pa = tf.layers.dense(a, units, activation=None)\n",
    "    pb = tf.layers.dense(b, units, activation=None)\n",
    "    c = pa + pb\n",
    "    if activation is not None:\n",
    "        c = activation(c)\n",
    "    return c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A slightly more general form of this function is [included](#merge) in the cookbook.\n",
    "\n",
    "So far we discussed the good part of broadcasting. But what’s the ugly part you may ask? Implicit assumptions almost always make debugging harder to do. Consider the following example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a = tf.constant([[1.], [2.]])\n",
    "b = tf.constant([1., 2.])\n",
    "c = tf.reduce_sum(a + b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do you think the value of c would be after evaluation? If you guessed 6, that’s wrong. It’s going to be 12. This is because when rank of two tensors don’t match, TensorFlow automatically expands the first dimension of the tensor with lower rank before the elementwise operation, so the result of addition would be [[2, 3], [3, 4]], and the reducing over all parameters would give us 12."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12.0"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "sess.run(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The way to avoid this problem is to be as explicit as possible. Had we specified which dimension we would want to reduce across, catching this bug would have been much easier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5., 7.], dtype=float32)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = tf.constant([[1.], [2.]])\n",
    "b = tf.constant([1., 2.])\n",
    "c = tf.reduce_sum(a + b, 0)\n",
    "sess.run(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here the value of c is [5, 7], and we immediately would guess based on the shape of the result that there’s something wrong. A general rule of thumb is to always specify the dimensions in reduction operations and when using tf.squeeze.\n",
    "\n",
    "## Feeding data to TensorFlow\n",
    "<a name=\"data\"></a>\n",
    "\n",
    "TensorFlow is designed to work efficiently with large amount of data. So it's important not to starve your TensorFlow model in order to maximize its performance. There are various ways that you can feed your data to TensorFlow.\n",
    "\n",
    "### Constants\n",
    "The simplest approach is to embed the data in your graph as a constant:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "actual_data = np.random.normal(size=[100])\n",
    "\n",
    "data = tf.constant(actual_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This approach can be very efficient, but it's not very flexible. One problem with this approach is that, in order to use your model with another dataset you have to rewrite the graph. Also, you have to load all of your data at once and keep it in memory which would only work with small datasets.\n",
    "\n",
    "### Placeholders\n",
    "Using placeholders solves both of these problems:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.0801002,  1.1062065,  1.0322781,  1.3388014,  2.137343 ,\n",
       "        2.144048 ,  1.1312735,  1.0975827,  1.1042416,  1.0713935,\n",
       "        2.137093 ,  1.0130063,  1.1272428,  2.972642 ,  2.145298 ,\n",
       "        1.2534629,  1.5266517,  1.0886548,  1.0015733,  1.0024785,\n",
       "        6.4612956,  1.4421113,  3.4781992,  1.1705618,  1.6153715,\n",
       "        1.643919 ,  1.4123268,  1.0319624,  1.6351577,  2.3298292,\n",
       "        4.938554 ,  1.2902929,  1.7381715,  3.0689952,  1.1804026,\n",
       "        1.2780612,  1.3359121,  2.2184563,  1.1397685,  1.0682502,\n",
       "        1.0504601,  1.7654779, 10.669445 ,  1.0391484,  1.2893654,\n",
       "        2.170648 ,  1.3763938,  1.0894359,  2.0194495,  1.8645837,\n",
       "        1.0002996,  1.0024247,  1.0451918,  1.4342833,  2.1268582,\n",
       "        1.2002347,  3.2516823,  2.2922962,  4.3227673,  1.0005083,\n",
       "        3.6290567,  1.9781268,  1.0097058,  1.2194831,  1.0983746,\n",
       "        1.4031519,  5.3541427,  1.4163513,  1.0234586,  1.8074186,\n",
       "        1.0269774,  3.8006759,  1.219772 ,  1.131836 ,  2.6998982,\n",
       "        1.1944921,  1.6550295,  1.0001546,  7.5599575,  2.9871073,\n",
       "        1.5160005,  2.418665 ,  1.1118016,  4.5607586,  1.0523133,\n",
       "        1.600655 ,  1.2016807,  1.0322464,  5.3959975,  2.264629 ,\n",
       "        1.0134735,  1.0651246,  2.1379604,  4.0274267,  2.9998717,\n",
       "        1.0292094,  1.2835612,  4.4727955,  3.0787656,  2.0937955],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = tf.placeholder(tf.float32)\n",
    "\n",
    "prediction = tf.square(data) + 1\n",
    "\n",
    "actual_data = np.random.normal(size=[100])\n",
    "\n",
    "tf.Session().run(prediction, feed_dict={data: actual_data})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Placeholder operator returns a tensor whose value is fetched through the feed_dict argument in Session.run function. Note that running Session.run without feeding the value of data in this case will result in an error.\n",
    "\n",
    "### Python ops\n",
    "Another approach to feed the data to TensorFlow is by using Python ops:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def py_input_fn():\n",
    "    actual_data = np.random.normal(size=[100])\n",
    "    return actual_data\n",
    "\n",
    "data = tf.py_func(py_input_fn, [], (tf.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Python ops allow you to convert a regular Python function to a TensorFlow operation.\n",
    "\n",
    "### Dataset API\n",
    "The recommended way of reading the data in TensorFlow however is through the dataset API:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "actual_data = np.random.normal(size=[100])\n",
    "dataset = tf.data.Dataset.from_tensor_slices(actual_data)\n",
    "data = dataset.make_one_shot_iterator().get_next()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you need to read your data from file, it may be more efficient to write it in TFrecord format and use TFRecordDataset to read it:\n",
    "\n",
    "```python\n",
    "dataset = tf.contrib.data.TFRecordDataset(path_to_data)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See the [official docs](https://www.tensorflow.org/api_guides/python/reading_data#Reading_from_files) for an example of how to write your dataset in TFrecord format.\n",
    "\n",
    "Dataset API allows you to make efficient data processing pipelines easily. For example this is how we process our data in the accompanied framework (See\n",
    "[trainer.py](https://github.com/vahidk/TensorflowFramework/blob/master/trainer.py)):\n",
    "\n",
    "```python\n",
    "dataset = ...\n",
    "dataset = dataset.cache()\n",
    "if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "    dataset = dataset.repeat()\n",
    "    dataset = dataset.shuffle(batch_size * 5)\n",
    "dataset = dataset.map(parse, num_threads=8)\n",
    "dataset = dataset.batch(batch_size)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After reading the data, we use Dataset.cache method to cache it into memory for improved efficiency. During the training mode, we repeat the dataset indefinitely. This allows us to process the whole dataset many times. We also shuffle the dataset to get batches with different sample distributions. Next, we use the Dataset.map function to perform preprocessing on raw records and convert the data to a usable format for the model. We then create batches of samples by calling Dataset.batch.\n",
    "\n",
    "## Take advantage of the overloaded operators\n",
    "<a name=\"overloaded_ops\"></a>\n",
    "Just like NumPy, TensorFlow overloads a number of python operators to make building graphs easier and the code more readable.\n",
    "\n",
    "The slicing op is one of the overloaded operators that can make indexing tensors very easy:\n",
    "```python\n",
    "z = x[begin:end]  # z = tf.slice(x, [begin], [end-begin])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Be very careful when using this op though. The slicing op is very inefficient and often better avoided, especially when the number of slices is high. To understand how inefficient this op can be let's look at an example. We want to manually perform reduction across the rows of a matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.44 s, sys: 12.9 ms, total: 1.46 s\n",
      "Wall time: 1.45 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([250.85672, 257.13156, 253.32951, 254.34499, 245.85526, 254.15256,\n",
       "       253.8952 , 255.99843, 252.47758, 248.35065], dtype=float32)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "\n",
    "x = tf.random_uniform([500, 10])\n",
    "\n",
    "z = tf.zeros([10])\n",
    "for i in range(500):\n",
    "    z += x[i]\n",
    "\n",
    "sess = tf.Session()\n",
    "\n",
    "%time sess.run(z);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On my MacBook Pro, this took 976 ms to run! The reason is that we are calling the slice op 500 times, which is going to be very slow to run. A better choice would have been to use tf.unstack op to slice the matrix into a list of vectors all at once:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 244 ms, sys: 0 ns, total: 244 ms\n",
      "Wall time: 243 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "z = tf.zeros([10])\n",
    "for x_i in tf.unstack(x):\n",
    "    z += x_i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This took 277 ms. Of course, the right way to do this simple reduction is to use tf.reduce_sum op:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.56 ms, sys: 0 ns, total: 2.56 ms\n",
      "Wall time: 2.24 ms\n"
     ]
    }
   ],
   "source": [
    "%time z = tf.reduce_sum(x, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This took 3.36 ms, which is ~300x faster than the original implementation.\n",
    "\n",
    "TensorFlow also overloads a range of arithmetic and logical operators:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = tf.random_uniform([500, 10])\n",
    "y = tf.random_uniform([500, 10])\n",
    "\n",
    "z = -x  # z = tf.negative(x)\n",
    "z = x + y  # z = tf.add(x, y)\n",
    "z = x - y  # z = tf.subtract(x, y)\n",
    "z = x * y  # z = tf.mul(x, y)\n",
    "z = x / y  # z = tf.div(x, y)\n",
    "z = x // y  # z = tf.floordiv(x, y)\n",
    "z = x % y  # z = tf.mod(x, y)\n",
    "z = x ** y  # z = tf.pow(x, y)\n",
    "z = x @ tf.transpose(y)  # z = tf.matmul(x, y)\n",
    "z = x > y  # z = tf.greater(x, y)\n",
    "z = x >= y  # z = tf.greater_equal(x, y)\n",
    "z = x < y  # z = tf.less(x, y)\n",
    "z = x <= y  # z = tf.less_equal(x, y)\n",
    "z = abs(x)  # z = tf.abs(x)\n",
    "\n",
    "\n",
    "# Now, for logical operations\n",
    "x = tf.constant([True, False])\n",
    "y = tf.constant([True, True])\n",
    "\n",
    "z = x & y  # z = tf.logical_and(x, y)\n",
    "z = x | y  # z = tf.logical_or(x, y)\n",
    "z = x ^ y  # z = tf.logical_xor(x, y)\n",
    "z = ~x  # z = tf.logical_not(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also use the augmented version of these ops. For example `x += y` and `x **= 2` are also valid.\n",
    "\n",
    "Note that Python doesn't allow overloading \"`and`\", \"`or`\", and \"`not`\" keywords.\n",
    "\n",
    "TensorFlow also doesn't allow using tensors as booleans, as it may be error prone:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = tf.constant(1.)\n",
    "# This would raise a TypeError error\n",
    "# if x: \n",
    "#    ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can either use tf.cond(x, ...) if you want to check the value of the tensor, or use \"if x is None\" to check the value of the variable.\n",
    "\n",
    "Other operators that aren't supported are equal (`==`) and not equal (`!=`) operators which are overloaded in NumPy but not in TensorFlow. Use the function versions instead which are `tf.equal` and `tf.not_equal`.\n",
    "\n",
    "\n",
    "## Understanding order of execution and control dependencies\n",
    "<a name=\"control_deps\"></a>\n",
    "As we discussed in the first item, TensorFlow doesn't immediately run the operations that are defined but rather creates corresponding nodes in a graph that can be evaluated with Session.run() method. This also enables TensorFlow to do optimizations at run time to determine the optimal order of execution and possible trimming of unused nodes. If you only have tf.Tensors in your graph you don't need to worry about dependencies but you most probably have tf.Variables too, and tf.Variables make things much more difficult. My advice to is to only use Variables if Tensors don't do the job. This might not make a lot of sense to you now, so let's start with an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = tf.constant(1)\n",
    "b = tf.constant(2)\n",
    "a = a + b\n",
    "\n",
    "sess.run(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluating \"a\" will return the value 3 as expected.  Note that here we are creating 3 tensors, two constant tensors and another tensor that stores the result of the addition. Note that you can't overwrite the value of a tensor. If you want to modify it you have to create a new tensor. As we did here.\n",
    "\n",
    "***\n",
    "__TIP__: If you don't define a new graph, TensorFlow automatically creates a graph for you by default. You can use tf.get_default_graph() to get a handle to the graph. You can then inspect the graph, for example by printing all its tensors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor 'or:0' shape=(2,) dtype=bool>,\n",
       " <tf.Tensor 'xor/LogicalOr:0' shape=(2,) dtype=bool>,\n",
       " <tf.Tensor 'xor/LogicalAnd:0' shape=(2,) dtype=bool>,\n",
       " <tf.Tensor 'xor/LogicalNot:0' shape=(2,) dtype=bool>,\n",
       " <tf.Tensor 'xor:0' shape=(2,) dtype=bool>,\n",
       " <tf.Tensor 'LogicalNot:0' shape=(2,) dtype=bool>,\n",
       " <tf.Tensor 'Const_10:0' shape=() dtype=float32>,\n",
       " <tf.Tensor 'Const_11:0' shape=() dtype=int32>,\n",
       " <tf.Tensor 'Const_12:0' shape=() dtype=int32>,\n",
       " <tf.Tensor 'add_1006:0' shape=() dtype=int32>]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The list of tensors defined so far is quite long, so I'm showing only\n",
    "# the last 10 elements\n",
    "tf.contrib.graph_editor.get_tensors(tf.get_default_graph())[-10:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unlike tensors, variables can be updated. So let's see how we may use variables to do the same thing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "a = tf.Variable(1)\n",
    "b = tf.constant(2)\n",
    "assign = tf.assign(a, a + b)\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "print(sess.run(assign))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, we get 3 as expected. Note that tf.assign returns a tensor representing the value of the assignment.\n",
    "So far everything seemed to be fine, but let's look at a slightly more complicated example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5, 7]\n",
      "[5, 7]\n",
      "[5, 7]\n",
      "[5, 7]\n",
      "[5, 3]\n",
      "[5, 7]\n",
      "[5, 3]\n",
      "[5, 7]\n",
      "[5, 7]\n",
      "[5, 7]\n"
     ]
    }
   ],
   "source": [
    "a = tf.Variable(1)\n",
    "b = tf.constant(2)\n",
    "c = a + b\n",
    "\n",
    "assign = tf.assign(a, 5)\n",
    "\n",
    "sess = tf.Session()\n",
    "for i in range(10):\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    print(sess.run([assign, c]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the tensor c here won't have a deterministic value. This value might be 3 or 7 depending on whether addition or assignment gets executed first.\n",
    "\n",
    "**You should note that the order that you define ops in your code doesn't matter to TensorFlow runtime.** The only thing that matters is the control dependencies. Control dependencies for tensors are straightforward. Every time you use a tensor in an operation that op will define an implicit dependency to that tensor. But things get complicated with variables because they can take many values.\n",
    "\n",
    "When dealing with variables, you may need to explicitly define dependencies using tf.control_dependencies() as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5, 3]\n",
      "[5, 3]\n",
      "[5, 3]\n",
      "[5, 3]\n",
      "[5, 3]\n",
      "[5, 3]\n",
      "[5, 3]\n",
      "[5, 3]\n",
      "[5, 3]\n",
      "[5, 3]\n"
     ]
    }
   ],
   "source": [
    "a = tf.Variable(1)\n",
    "b = tf.constant(2)\n",
    "c = a + b\n",
    "\n",
    "with tf.control_dependencies([c]):\n",
    "    assign = tf.assign(a, 5)\n",
    "\n",
    "sess = tf.Session()\n",
    "for i in range(10):\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    print(sess.run([assign, c]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will make sure that the assign op will be called after the addition.\n",
    "\n",
    "## Control flow operations: conditionals and loops\n",
    "<a name=\"control_flow\"></a>\n",
    "When building complex models such as recurrent neural networks you may need to control the flow of operations through conditionals and loops. In this section we introduce a number of commonly used control flow ops.\n",
    "\n",
    "Let's assume you want to decide whether to multiply to or add two given tensors based on a predicate. This can be simply implemented with tf.cond which acts as a python \"if\" function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "a = tf.constant(1)\n",
    "b = tf.constant(2)\n",
    "\n",
    "p = tf.constant(True)\n",
    "\n",
    "x = tf.cond(p, lambda: a + b, lambda: a * b)\n",
    "\n",
    "print(tf.Session().run(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the predicate is True in this case, the output would be the result of the addition, which is 3.\n",
    "\n",
    "Most of the times when using TensorFlow you are using large tensors and want to perform operations in batch. A related conditional operation is tf.where, which like tf.cond takes a predicate, but selects the output based on the condition in batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3 2]\n"
     ]
    }
   ],
   "source": [
    "a = tf.constant([1, 1])\n",
    "b = tf.constant([2, 2])\n",
    "\n",
    "p = tf.constant([True, False])\n",
    "\n",
    "x = tf.where(p, a + b, a * b)  # Similar to np.where\n",
    "\n",
    "print(tf.Session().run(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another widely used control flow operation is tf.while_loop. It allows building dynamic loops in TensorFlow that operate on sequences of variable length. Let's see how we can generate Fibonacci sequence with tf.while_loops:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    }
   ],
   "source": [
    "n = tf.constant(5)\n",
    "\n",
    "\n",
    "def cond(i, a, b):\n",
    "    return i < n\n",
    "\n",
    "\n",
    "def body(i, a, b):\n",
    "    return i + 1, b, a + b\n",
    "\n",
    "\n",
    "i, a, b = tf.while_loop(cond, body, (2, 1, 1))\n",
    "\n",
    "print(tf.Session().run(b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will print 5. tf.while_loops takes a condition function, and a loop body function, in addition to initial values for loop variables. These loop variables are then updated by multiple calls to the body function until the condition returns false.\n",
    "\n",
    "Now imagine we want to keep the whole series of Fibonacci sequence. We may update our body to keep a record of the history of current values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n = tf.constant(5)\n",
    "\n",
    "def cond(i, a, b, c):\n",
    "    return i < n\n",
    "\n",
    "def body(i, a, b, c):\n",
    "    return i + 1, b, a + b, tf.concat([c, [a + b]], 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now if you try running this:\n",
    "```python\n",
    "i, a, b, c = tf.while_loop(cond, body, (2, 1, 1, tf.constant([1, 1])))\n",
    "print(tf.Session().run(c))\n",
    "```\n",
    "TensorFlow will complain that the shape of the the fourth loop variable is changing. So you must make that explicit that it's intentional:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "i, a, b, c = tf.while_loop(\n",
    "    cond, body, (2, 1, 1, tf.constant([1, 1])),\n",
    "    shape_invariants=(\n",
    "        tf.TensorShape([]),\n",
    "        tf.TensorShape([]),\n",
    "        tf.TensorShape([]),\n",
    "        tf.TensorShape([None])\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is not only getting ugly, but is also somewhat inefficient. Note that we are building a lot of intermediary tensors that we don't use. TensorFlow has a better solution for this kind of growing arrays. Meet tf.TensorArray. Let's do the same thing this time with tensor arrays:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 2 3 5]\n"
     ]
    }
   ],
   "source": [
    "n = tf.constant(5)\n",
    "\n",
    "c = tf.TensorArray(tf.int32, n)\n",
    "c = c.write(0, 1)\n",
    "c = c.write(1, 1)\n",
    "\n",
    "def cond(i, a, b, c):\n",
    "    return i < n\n",
    "\n",
    "def body(i, a, b, c):\n",
    "    c = c.write(i, a + b)\n",
    "    return i + 1, b, a + b, c\n",
    "\n",
    "\n",
    "i, a, b, c = tf.while_loop(cond, body, (2, 1, 1, c))\n",
    "\n",
    "c = c.stack()\n",
    "\n",
    "print(tf.Session().run(c))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TensorFlow while loops and tensor arrays are essential tools for building complex recurrent neural networks. As an exercise try implementing [beam search](https://en.wikipedia.org/wiki/Beam_search) using tf.while_loops. Can you make it more efficient with tensor arrays?\n",
    "\n",
    "## Prototyping kernels and advanced visualization with Python ops\n",
    "<a name=\"python_ops\"></a>\n",
    "Operation kernels in TensorFlow are entirely written in C++ for efficiency. But writing a TensorFlow kernel in C++ can be quite a pain. So, before spending hours implementing your kernel you may want to prototype something quickly, however inefficient. With tf.py_func() you can turn any piece of python code to a TensorFlow operation.\n",
    "\n",
    "For example this is how you can implement a simple ReLU nonlinearity kernel in TensorFlow as a python op:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import uuid\n",
    "\n",
    "\n",
    "def relu(inputs):\n",
    "    # Define the op in python\n",
    "    def _relu(x):\n",
    "        return np.maximum(x, 0.)\n",
    "\n",
    "    # Define the op's gradient in python\n",
    "    def _relu_grad(x):\n",
    "        return np.float32(x > 0)\n",
    "\n",
    "    # An adapter that defines a gradient op compatible with TensorFlow\n",
    "    def _relu_grad_op(op, grad):\n",
    "        x = op.inputs[0]\n",
    "        x_grad = grad * tf.py_func(_relu_grad, [x], tf.float32)\n",
    "        return x_grad\n",
    "\n",
    "    # Register the gradient with a unique id\n",
    "    grad_name = \"MyReluGrad_\" + str(uuid.uuid4())\n",
    "    tf.RegisterGradient(grad_name)(_relu_grad_op)\n",
    "\n",
    "    # Override the gradient of the custom op\n",
    "    g = tf.get_default_graph()\n",
    "    with g.gradient_override_map({\"PyFunc\": grad_name}):\n",
    "        output = tf.py_func(_relu, [inputs], tf.float32)\n",
    "        \n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To verify that the gradients are correct you can use TensorFlow's gradient checker:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.790855407714844e-05\n"
     ]
    }
   ],
   "source": [
    "x = tf.random_normal([10])\n",
    "y = relu(x * x)\n",
    "\n",
    "with tf.Session():\n",
    "    diff = tf.test.compute_gradient_error(x, [10], y, [10])\n",
    "    print(diff)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "compute_gradient_error() computes the gradient numerically and returns the difference with the provided gradient. What we want is a very low difference.\n",
    "\n",
    "Note that this implementation is pretty inefficient, and is only useful for prototyping, since the python code is not parallelizable and won't run on GPU. Once you verified your idea, you definitely would want to write it as a C++ kernel.\n",
    "\n",
    "In practice we commonly use python ops to do visualization on Tensorboard. Consider the case that you are building an image classification model and want to visualize your model predictions during training. TensorFlow allows visualizing images with tf.summary.image() function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'image:0' shape=() dtype=string>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image = tf.placeholder(tf.float32)\n",
    "tf.summary.image(\"image\", image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But this only visualizes the input image. In order to visualize the predictions you have to find a way to add annotations to the image which may be almost impossible with existing ops. An easier way to do this is to do the drawing in python, and wrap it in a python op:\n",
    "```python\n",
    "import io\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import PIL\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "def visualize_labeled_images(images, labels, max_outputs=3, name=\"image\"):\n",
    "    def _visualize_image(image, label):\n",
    "        # Do the actual drawing in python\n",
    "        fig = plt.figure(figsize=(3, 3), dpi=80)\n",
    "        ax = fig.add_subplot(111)\n",
    "        ax.imshow(image[::-1,...])\n",
    "        ax.text(0, 0, str(label),\n",
    "          horizontalalignment=\"left\",\n",
    "          verticalalignment=\"top\")\n",
    "        fig.canvas.draw()\n",
    "\n",
    "        # Write the plot as a memory file.\n",
    "        buf = io.BytesIO()\n",
    "        data = fig.savefig(buf, format=\"png\")\n",
    "        buf.seek(0)\n",
    "\n",
    "        # Read the image and convert to numpy array\n",
    "        img = PIL.Image.open(buf)\n",
    "        return np.array(img.getdata()).reshape(img.size[0], img.size[1], -1)\n",
    "\n",
    "    def _visualize_images(images, labels):\n",
    "        # Only display the given number of examples in the batch\n",
    "        outputs = []\n",
    "        for i in range(max_outputs):\n",
    "            output = _visualize_image(images[i], labels[i])\n",
    "            outputs.append(output)\n",
    "        return np.array(outputs, dtype=np.uint8)\n",
    "\n",
    "    # Run the python op.\n",
    "    figs = tf.py_func(_visualize_images, [images, labels], tf.uint8)\n",
    "    return tf.summary.image(name, figs)\n",
    "```\n",
    "\n",
    "Note that since summaries are usually only evaluated once in a while (not per step), this implementation may be used in practice without worrying about efficiency.\n",
    "\n",
    "## Multi-GPU processing with data parallelism\n",
    "<a name=\"multi_gpu\"></a>\n",
    " If you write your software in a language like C++ for a single cpu core, making it run on multiple GPUs in parallel would require rewriting the software from scratch. But this is not the case with TensorFlow. Because of its symbolic nature, tensorflow can hide all that complexity, making it effortless to scale your program across many CPUs and GPUs.\n",
    "\n",
    " Let's start with the simple example of adding two vectors on CPU:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.1216325 , 1.1862247 , 0.99181306, ..., 0.95810926, 1.0323286 ,\n",
       "        1.1293956 ],\n",
       "       [1.2750984 , 0.93556595, 1.0815791 , ..., 1.780375  , 1.4725544 ,\n",
       "        0.99033964],\n",
       "       [1.0654018 , 0.91482747, 0.85143816, ..., 0.69220316, 1.4187143 ,\n",
       "        1.2036579 ],\n",
       "       ...,\n",
       "       [0.52804637, 1.0250583 , 1.1928155 , ..., 0.68355346, 1.6651546 ,\n",
       "        1.9402959 ],\n",
       "       [1.5417502 , 1.0824977 , 0.5608696 , ..., 0.49354672, 0.8076316 ,\n",
       "        0.561182  ],\n",
       "       [0.91508496, 0.79619014, 0.6022583 , ..., 1.6610327 , 1.8229514 ,\n",
       "        0.93388057]], dtype=float32)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with tf.device(tf.DeviceSpec(device_type=\"CPU\", device_index=0)):\n",
    "    a = tf.random_uniform([1000, 100])\n",
    "    b = tf.random_uniform([1000, 100])\n",
    "    c = a + b\n",
    "\n",
    "tf.Session().run(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The same thing can as simply be done on GPU:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.device(tf.DeviceSpec(device_type=\"GPU\", device_index=0)):\n",
    "    a = tf.random_uniform([1000, 100])\n",
    "    b = tf.random_uniform([1000, 100])\n",
    "    c = a + b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But what if we have two GPUs and want to utilize both? To do that, we can split the data and use a separate GPU for processing each half:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "split_a = tf.split(a, 2)\n",
    "split_b = tf.split(b, 2)\n",
    "\n",
    "split_c = []\n",
    "for i in range(2):\n",
    "    with tf.device(tf.DeviceSpec(device_type=\"GPU\", device_index=i)):\n",
    "        split_c.append(split_a[i] + split_b[i])\n",
    "\n",
    "c = tf.concat(split_c, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's rewrite this in a more general form so that we can replace addition with any other set of operations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_parallel(fn, num_gpus, **kwargs):\n",
    "    in_splits = {}\n",
    "    for k, v in kwargs.items():\n",
    "        in_splits[k] = tf.split(v, num_gpus)\n",
    "\n",
    "    out_split = []\n",
    "    for i in range(num_gpus):\n",
    "        with tf.device(tf.DeviceSpec(device_type=\"GPU\", device_index=i)):\n",
    "            with tf.variable_scope(tf.get_variable_scope(), reuse=i > 0):\n",
    "                out_split.append(fn(**{k : v[i] for k, v in in_splits.items()}))\n",
    "\n",
    "    return tf.concat(out_split, axis=0)\n",
    "\n",
    "\n",
    "def model(a, b):\n",
    "    return a + b\n",
    "\n",
    "c = make_parallel(model, 2, a=a, b=b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can replace the `model` with any function that takes a set of tensors as input and returns a tensor as result with the condition that both the input and output are in batch. Note that we also added a variable scope and set the reuse to true. This makes sure that we use the same variables for processing both splits. This is something that will become handy in our next example.\n",
    "\n",
    "Let's look at a slightly more practical example. We want to train a neural network on multiple GPUs. During training we not only need to compute the forward pass but also need to compute the backward pass (the gradients). But how can we parallelize the gradient computation? This turns out to be pretty easy.\n",
    "\n",
    "Recall from the first item that we wanted to fit a second degree polynomial to a set of samples. We reorganized the code a bit to have the bulk of the operations in the model function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[ 4.9984317e+00],\n",
      "       [-3.2114194e-04],\n",
      "       [ 3.0927134e+00]], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "\n",
    "def model(x, y):\n",
    "    w = tf.get_variable(\"w\", shape=[3, 1])\n",
    "\n",
    "    f = tf.stack([tf.square(x), x, tf.ones_like(x)], 1)\n",
    "    yhat = tf.squeeze(tf.matmul(f, w), 1)\n",
    "\n",
    "    loss = tf.square(yhat - y)\n",
    "    return loss\n",
    "\n",
    "\n",
    "x = tf.placeholder(tf.float32)\n",
    "y = tf.placeholder(tf.float32)\n",
    "\n",
    "loss = model(x, y)\n",
    "\n",
    "train_op = tf.train.AdamOptimizer(0.1).minimize(\n",
    "    tf.reduce_mean(loss))\n",
    "\n",
    "\n",
    "def generate_data():\n",
    "    x_val = np.random.uniform(-10.0, 10.0, size=100)\n",
    "    y_val = 5 * np.square(x_val) + 3\n",
    "    return x_val, y_val\n",
    "\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "for _ in range(1000):\n",
    "    x_val, y_val = generate_data()\n",
    "    _, loss_val = sess.run([train_op, loss], {x: x_val, y: y_val})\n",
    "\n",
    "_, loss_val = sess.run([train_op, loss], {x: x_val, y: y_val})\n",
    "print(sess.run(tf.contrib.framework.get_variables_by_name(\"w\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's use make_parallel that we just wrote to parallelize this. We only need to change two lines of code from the above code:\n",
    "```python\n",
    "loss = make_parallel(model, 2, x=x, y=y)\n",
    "\n",
    "train_op = tf.train.AdamOptimizer(0.1).minimize(\n",
    "    tf.reduce_mean(loss),\n",
    "    colocate_gradients_with_ops=True)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The only thing that we need to change to parallelize backpropagation of gradients is to set the colocate_gradients_with_ops flag to true. This ensures that gradient ops run on the same device as the original op.\n",
    "\n",
    "## Debugging TensorFlow models\n",
    "<a name=\"debug\"></a>\n",
    "Symbolic nature of TensorFlow makes it relatively more difficult to debug TensorFlow code compared to regular python code. Here we introduce a number of tools included with TensorFlow that make debugging much easier.\n",
    "\n",
    "Probably the most common error one can make when using TensorFlow is passing Tensors of wrong shape to ops. Many TensorFlow ops can operate on tensors of different ranks and shapes. This can be convenient when using the API, but may lead to extra headache when things go wrong.\n",
    "\n",
    "For example, consider the tf.matmul op, it can multiply two matrices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a = tf.random_uniform([2, 3])\n",
    "b = tf.random_uniform([3, 4])\n",
    "c = tf.matmul(a, b)  # c is a tensor of shape [2, 4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But the same function also does batch matrix multiplication:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'MatMul_2:0' shape=(10, 2, 4) dtype=float32>"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = tf.random_uniform([10, 2, 3])\n",
    "b = tf.random_uniform([10, 3, 4])\n",
    "tf.matmul(a, b)  # c is a tensor of shape [10, 2, 4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another example that we talked about before in the [broadcasting](#broadcast) section is add operation which supports broadcasting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a = tf.constant([[1.], [2.]])\n",
    "b = tf.constant([1., 2.])\n",
    "c = a + b  # c is a tensor of shape [2, 2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validating your tensors with tf.assert* ops\n",
    "\n",
    "One way to reduce the chance of unwanted behavior is to explicitly verify the rank or shape of intermediate tensors with `tf.assert_*()` ops."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ValueError: .  Tensor Const_2:0 must have rank 1.  Received rank 2, shape (2, 1)\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    a = tf.constant([[1.], [2.]])\n",
    "    b = tf.constant([1., 2.])\n",
    "    check_a = tf.assert_rank(a, 1)  # This will raise ValueError\n",
    "    check_b = tf.assert_rank(b, 1)\n",
    "    with tf.control_dependencies([check_a, check_b]):\n",
    "        c = a + b  # c is a tensor of shape [2, 2]\n",
    "except ValueError as e:\n",
    "    print('ValueError: {}'.format(e))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember that assertion nodes like other operations are part of the graph and if not evaluated would get pruned during Session.run(). So make sure to create explicit dependencies to assertion ops, to force TensorFlow to execute them.\n",
    "\n",
    "You can also use assertions to validate the value of tensors at runtime:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "check_pos = tf.assert_positive(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See the official docs for a [full list of assertion ops](https://www.tensorflow.org/api_guides/python/check_ops).\n",
    "\n",
    "### Logging tensor values with tf.Print\n",
    "\n",
    "Another useful built-in function for debugging is tf.Print which logs the given tensors to the standard error:\n",
    "```python\n",
    "input_copy = tf.Print(input, tensors_to_print_list)\n",
    "```\n",
    "\n",
    "Note that tf.Print returns a copy of its first argument as output. One way to force tf.Print to run is to pass its output to another op that gets executed. For example if we want to print the value of tensors a and b before adding them we could do something like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = tf.constant(2)\n",
    "b = tf.constant(5)\n",
    "\n",
    "a = tf.Print(a, [a, b])\n",
    "c = a + b\n",
    "\n",
    "sess.run(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively we could manually define a control dependency.\n",
    "\n",
    "### Check your gradients with tf.compute_gradient_error\n",
    "\n",
    "__Not__ all the operations in TensorFlow come with gradients, and it's easy to unintentionally build graphs for which TensorFlow can not compute the gradients.\n",
    "\n",
    "Let's look at an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-63-27b0b1a14d59>:3: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See tf.nn.softmax_cross_entropy_with_logits_v2.\n",
      "\n",
      "[0.17596693 0.17143957 0.23968036 0.2833984  0.12951475]\n"
     ]
    }
   ],
   "source": [
    "def non_differentiable_softmax_entropy(logits):\n",
    "    probs = tf.nn.softmax(logits)\n",
    "    return tf.nn.softmax_cross_entropy_with_logits(labels=probs, logits=logits)\n",
    "\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "w = tf.get_variable(\"w\", shape=[5])\n",
    "y = -non_differentiable_softmax_entropy(w)\n",
    "\n",
    "opt = tf.train.AdamOptimizer()\n",
    "train_op = opt.minimize(y)\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "for i in range(10000):\n",
    "    sess.run(train_op)\n",
    "\n",
    "print(sess.run(tf.nn.softmax(w)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are using tf.nn.softmax_cross_entropy_with_logits to define entropy over a categorical distribution. We then use Adam optimizer to find the weights with maximum entropy. If you have passed a course on information theory, you would know that uniform distribution contains maximum entropy. So you would expect for the result to be [0.2, 0.2, 0.2, 0.2, 0.2]. But if you run this you may get unexpected results like this:\n",
    "```\n",
    "[ 0.34081486  0.24287023  0.23465775  0.08935683  0.09230034]\n",
    "```\n",
    "It turns out tf.nn.softmax_cross_entropy_with_logits has undefined gradients with respect to labels! But how may we spot this if we didn't know?\n",
    "\n",
    "Fortunately for us TensorFlow comes with a numerical differentiator that can be used to find symbolic gradient errors. Let's see how we can use it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.10180472582578659\n"
     ]
    }
   ],
   "source": [
    "with tf.Session():\n",
    "    diff = tf.test.compute_gradient_error(w, [5], y, [])\n",
    "    print(diff)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you run this, you would see that the difference between the numerical and symbolic gradients are pretty high (0.06 - 0.1 in my tries).\n",
    "\n",
    "Now let's fix our function with a differentiable version of the entropy and check again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5,)\n",
      "()\n",
      "WARNING:tensorflow:From /home/md/anaconda3/envs/EffectiveTensorflow/lib/python3.6/site-packages/tensorflow/python/ops/nn_grad.py:249: calling reduce_sum (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "9.66787338256836e-05\n"
     ]
    }
   ],
   "source": [
    "def softmax_entropy(logits, dim=-1):\n",
    "    plogp = tf.nn.softmax(logits, dim) * tf.nn.log_softmax(logits, dim)\n",
    "    return -tf.reduce_sum(plogp, dim)\n",
    "\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "w = tf.get_variable(\"w\", shape=[5])\n",
    "y = -softmax_entropy(w)\n",
    "\n",
    "print(w.get_shape())\n",
    "print(y.get_shape())\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    diff = tf.test.compute_gradient_error(w, [5], y, [])\n",
    "    print(diff)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The difference should be ~0.0001 which looks much better.\n",
    "\n",
    "Now if you run the optimizer again with the correct version you can see the final weights would be:\n",
    "```\n",
    "[ 0.2  0.2  0.2  0.2  0.2]\n",
    "```\n",
    "which are exactly what we wanted.\n",
    "\n",
    "[TensorFlow summaries](https://www.tensorflow.org/api_guides/python/summary), and [tfdbg (TensorFlow Debugger)](https://www.tensorflow.org/api_guides/python/tfdbg) are other tools that can be used for debugging. Please refer to the official docs to learn more.\n",
    "\n",
    "## Numerical stability in TensorFlow\n",
    "<a name=\"stable\"></a>\n",
    "When using any numerical computation library such as NumPy or TensorFlow, it's important to note that writing mathematically correct code doesn't necessarily lead to correct results. You also need to make sure that the computations are stable.\n",
    "\n",
    "Let's start with a simple example. From primary school we know that x * y / y is equal to x for any non zero value of x. But let's see if that's always true in practice:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/md/anaconda3/envs/EffectiveTensorflow/lib/python3.6/site-packages/ipykernel_launcher.py:4: RuntimeWarning: invalid value encountered in float_scalars\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "x = np.float32(1)\n",
    "\n",
    "y = np.float32(1e-50)  # y would be stored as zero\n",
    "z = x * y / y\n",
    "\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The reason for the incorrect result is that y is simply too small for float32 type. A similar problem occurs when y is too large:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/md/anaconda3/envs/EffectiveTensorflow/lib/python3.6/site-packages/ipykernel_launcher.py:2: RuntimeWarning: invalid value encountered in float_scalars\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "y = np.float32(1e39)  # y would be stored as inf\n",
    "z = x * y / y\n",
    "\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The smallest positive value that float32 type can represent is 1.4013e-45 and anything below that would be stored as zero. Also, any number beyond 3.40282e+38, would be stored as inf:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1e-45\n",
      "3.4028235e+38\n"
     ]
    }
   ],
   "source": [
    "print(np.nextafter(np.float32(0), np.float32(1)))\n",
    "print(np.finfo(np.float32).max)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make sure that your computations are stable, you want to avoid values with small or very large absolute value. This may sound very obvious, but these kind of problems can become extremely hard to debug especially when doing gradient descent in TensorFlow. This is because you not only need to make sure that all the values in the forward pass are within the valid range of your data types, but also you need to make sure of the same for the backward pass (during gradient computation).\n",
    "\n",
    "Let's look at a real example. We want to compute the softmax over a vector of logits. A naive implementation would look something like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([nan,  0.], dtype=float32)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def unstable_softmax(logits):\n",
    "    exp = tf.exp(logits)\n",
    "    return exp / tf.reduce_sum(exp)\n",
    "\n",
    "\n",
    "tf.Session().run(unstable_softmax([1000., 0.]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that computing the exponential of logits for relatively small numbers results to gigantic results that are out of float32 range. The largest valid logit for our naive softmax implementation is ln(3.40282e+38) = 88.7, anything beyond that leads to a `nan` outcome.\n",
    "\n",
    "But how can we make this more stable? The solution is rather simple. It's easy to see that\n",
    "$$\\exp(x_j - c) \\big/ \\sum_i \\exp(x_i - c) = \\exp(x_j) \\big/ \\sum_i \\exp(x_i).$$\n",
    "Therefore we can subtract any constant from the logits and the result would remain the same. We choose this constant to be the maximum of logits. This way the domain of the exponential function would be limited to `[-inf, 0]`, and consequently its range would be `[0, 1]` which is desirable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 0.], dtype=float32)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def softmax(logits):\n",
    "    exp = tf.exp(logits - tf.reduce_max(logits))\n",
    "    return exp / tf.reduce_sum(exp)\n",
    "\n",
    "\n",
    "tf.Session().run(softmax([1000., 0.]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at a more complicated case. Consider we have a classification problem. We use the softmax function to produce probabilities from our logits. We then define our loss function to be the cross entropy between our predictions and the labels. Recall that cross entropy for a categorical distribution can be simply defined as\n",
    "$$\\text{xe}(p, q) = -\\sum_i p_i \\log q_i.$$\n",
    "So a naive implementation of the cross entropy would look like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inf\n"
     ]
    }
   ],
   "source": [
    "def unstable_softmax_cross_entropy(labels, logits):\n",
    "    logits = tf.log(softmax(logits))\n",
    "    return -tf.reduce_sum(labels * logits)\n",
    "\n",
    "\n",
    "labels = tf.constant([0.5, 0.5])\n",
    "logits = tf.constant([1000., 0.])\n",
    "\n",
    "xe = unstable_softmax_cross_entropy(labels, logits)\n",
    "\n",
    "print(tf.Session().run(xe))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that in this implementation as the softmax output approaches zero, the log's output approaches infinity which causes instability in our computation. We can rewrite this by expanding the softmax and doing some simplifications:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500.0\n"
     ]
    }
   ],
   "source": [
    "def softmax_cross_entropy(labels, logits):\n",
    "    scaled_logits = logits - tf.reduce_max(logits)\n",
    "    normalized_logits = scaled_logits - tf.reduce_logsumexp(scaled_logits)\n",
    "    return -tf.reduce_sum(labels * normalized_logits)\n",
    "\n",
    "\n",
    "labels = tf.constant([0.5, 0.5])\n",
    "logits = tf.constant([1000., 0.])\n",
    "\n",
    "xe = softmax_cross_entropy(labels, logits)\n",
    "\n",
    "print(tf.Session().run(xe))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also verify that the gradients are also computed correctly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([ 0.5, -0.5], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "g = tf.gradients(xe, logits)\n",
    "print(tf.Session().run(g))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "which is correct.\n",
    "\n",
    "Let me remind again that extra care must be taken when doing gradient descent to make sure that the range of your functions as well as the gradients for each layer are within a valid range. Exponential and logarithmic functions when used naively are especially problematic because they can map small numbers to enormous ones and the other way around.\n",
    "\n",
    "## Building a neural network training framework with learn API\n",
    "<a name=\"tf_learn\"></a>\n",
    "For simplicity, in most of the examples here we manually create sessions and we don't care about saving and loading checkpoints but this is not how we usually do things in practice. You most probably want to use the learn API to take care of session management and logging. We provide a simple but practical [framework](https://github.com/vahidk/TensorflowFramework/tree/master) for training neural networks using TensorFlow. In this item we explain how this framework works.\n",
    "\n",
    "When experimenting with neural network models you usually have a training/test split. You want to train your model on the training set, and once in a while evaluate it on test set and compute some metrics. You also need to store the model parameters as a checkpoint, and ideally you want to be able to stop and resume training. TensorFlow's learn API is designed to make this job easier, letting us focus on developing the actual model.\n",
    "\n",
    "The most basic way of using tf.learn API is to use tf.Estimator object directly. You need to define a model function that defines a loss function, a train op, one or a set of predictions, and optionally a set of metric ops for evaluation:\n",
    "```python\n",
    "def model_fn(features, labels, mode, params):\n",
    "    predictions = ...\n",
    "    loss = ...\n",
    "    train_op = ...\n",
    "    metric_ops = ...\n",
    "    return tf.estimator.EstimatorSpec(\n",
    "        mode=mode,\n",
    "        predictions=predictions,\n",
    "        loss=loss,\n",
    "        train_op=train_op,\n",
    "        eval_metric_ops=metric_ops)\n",
    "\n",
    "\n",
    "params = ...\n",
    "run_config = tf.contrib.learn.RunConfig(model_dir=FLAGS.output_dir)\n",
    "estimator = tf.estimator.Estimator(\n",
    "    model_fn=model_fn, config=run_config, params=params)\n",
    "```\n",
    "\n",
    "\n",
    "To train the model you would then simply call Estimator.train() function while providing an input function to read the data:\n",
    "```python\n",
    "def input_fn():\n",
    "    features = ...\n",
    "    labels = ...\n",
    "    return features, labels\n",
    "\n",
    "estimator.train(input_fn=input_fn, max_steps=...)\n",
    "```\n",
    "\n",
    "and to evaluate the model, simply call Estimator.evaluate():\n",
    "```\n",
    "estimator.evaluate(input_fn=input_fn)\n",
    "```\n",
    "\n",
    "Estimator object might be good enough for simple cases, but TensorFlow provides a higher level object called Experiment which provides some additional useful functionality. Creating an experiment object is very easy:\n",
    "\n",
    "```python\n",
    "experiment = tf.contrib.learn.Experiment(\n",
    "    estimator=estimator,\n",
    "    train_input_fn=train_input_fn,\n",
    "    eval_input_fn=eval_input_fn)\n",
    "```\n",
    "\n",
    "Now we can call train_and_evaluate function to compute the metrics while training:\n",
    "```\n",
    "experiment.train_and_evaluate()\n",
    "```\n",
    "\n",
    "---\n",
    "An even higher level way of running experiments is by using learn_runner.run() function. Here's how our main function looks like in the provided framework:\n",
    "```python\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "tf.flags.DEFINE_string(\"output_dir\", \"\", \"Optional output dir.\")\n",
    "tf.flags.DEFINE_string(\"schedule\", \"train_and_evaluate\", \"Schedule.\")\n",
    "tf.flags.DEFINE_string(\"hparams\", \"\", \"Hyper parameters.\")\n",
    "\n",
    "FLAGS = tf.flags.FLAGS\n",
    "\n",
    "\n",
    "def experiment_fn(run_config, hparams):\n",
    "  estimator = tf.estimator.Estimator(\n",
    "    model_fn=make_model_fn(),\n",
    "    config=run_config,\n",
    "    params=hparams)\n",
    "  return tf.contrib.learn.Experiment(\n",
    "    estimator=estimator,\n",
    "    train_input_fn=make_input_fn(tf.estimator.ModeKeys.TRAIN, hparams),\n",
    "    eval_input_fn=make_input_fn(tf.estimator.ModeKeys.EVAL, hparams))\n",
    "\n",
    "\n",
    "def main(unused_argv):\n",
    "  run_config = tf.contrib.learn.RunConfig(model_dir=FLAGS.output_dir)\n",
    "  hparams = tf.contrib.training.HParams()\n",
    "  hparams.parse(FLAGS.hparams)\n",
    "\n",
    "  estimator = tf.contrib.learn.learn_runner.run(\n",
    "    experiment_fn=experiment_fn,\n",
    "    run_config=run_config,\n",
    "    schedule=FLAGS.schedule,\n",
    "    hparams=hparams)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "  tf.app.run()\n",
    "```\n",
    "\n",
    "The `\"schedule\"` flag decides which member function of the Experiment object gets called. So, if you for example set schedule to \"train_and_evaluate\", `experiment.train_and_evaluate()` would be called.\n",
    "\n",
    "The input function returns two tensors (or dictionaries of tensors) providing the features and labels to be passed to the model:\n",
    "```python\n",
    "def input_fn():\n",
    "    features = ...\n",
    "    labels = ...\n",
    "    return features, labels\n",
    "```\n",
    "See [mnist.py](https://github.com/vahidk/TensorflowFramework/blob/master/dataset/mnist.py) for an example of how to read your data with the dataset API. To learn about various ways of reading your data in TensorFlow refer to [this item](#data).\n",
    "\n",
    "The framework also comes with a simple convolutional network classifier in [alexnet.py](https://github.com/vahidk/TensorflowFramework/blob/master/model/alexnet.py) that includes an example model.\n",
    "\n",
    "And that's it! This is all you need to get started with TensorFlow learn API. I recommend to have a look at the framework [source code](https://github.com/vahidk/TensorFlowFramework) and see the official python API to learn more about the learn API.\n",
    "\n",
    "\n",
    "## TensorFlow Cookbook\n",
    "<a name=\"cookbook\"></a>\n",
    "This section includes implementation of a set of common operations in TensorFlow.\n",
    "\n",
    "### Get shape <a name=\"get_shape\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_shape(tensor):\n",
    "    \"\"\"Returns static shape if available and dynamic shape otherwise.\"\"\"\n",
    "    static_shape = tensor.shape.as_list()\n",
    "    dynamic_shape = tf.unstack(tf.shape(tensor))\n",
    "    dims = [s[1] if s[0] is None else s[0]\n",
    "            for s in zip(static_shape, dynamic_shape)]\n",
    "    return dims"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch Gather <a name=\"batch_gather\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def batch_gather(tensor, indices):\n",
    "    \"\"\"Gather in batch from a tensor of arbitrary size.\n",
    "\n",
    "    In pseudocode this module will produce the following:\n",
    "    output[i] = tf.gather(tensor[i], indices[i])\n",
    "\n",
    "    Args:\n",
    "        tensor: Tensor of arbitrary size.\n",
    "        indices: Vector of indices.\n",
    "    Returns:\n",
    "        output: A tensor of gathered values.\n",
    "    \"\"\"\n",
    "    shape = get_shape(tensor)\n",
    "    flat_first = tf.reshape(tensor, [shape[0] * shape[1]] + shape[2:])\n",
    "    indices = tf.convert_to_tensor(indices)\n",
    "    offset_shape = [shape[0]] + [1] * (indices.shape.ndims - 1)\n",
    "    offset = tf.reshape(tf.range(shape[0]) * shape[1], offset_shape)\n",
    "    output = tf.gather(flat_first, indices + offset)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Beam Search <a name=\"beam_search\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def rnn_beam_search(update_fn, initial_state, sequence_length, beam_width,\n",
    "                    begin_token_id, end_token_id, name=\"rnn\"):\n",
    "    \"\"\"Beam-search decoder for recurrent models.\n",
    "\n",
    "    Args:\n",
    "        update_fn: Function to compute the next state and logits given the current\n",
    "            state and ids.\n",
    "        initial_state: Recurrent model states.\n",
    "        sequence_length: Length of the generated sequence.\n",
    "        beam_width: Beam width.\n",
    "        begin_token_id: Begin token id.\n",
    "        end_token_id: End token id.\n",
    "        name: Scope of the variables.\n",
    "    Returns:\n",
    "        ids: Output indices.\n",
    "        logprobs: Output log probabilities probabilities.\n",
    "    \"\"\"\n",
    "    batch_size = initial_state.shape.as_list()[0]\n",
    "\n",
    "    state = tf.tile(tf.expand_dims(initial_state, axis=1), [1, beam_width, 1])\n",
    "\n",
    "    sel_sum_logprobs = tf.log([[1.] + [0.] * (beam_width - 1)])\n",
    "\n",
    "    ids = tf.tile([[begin_token_id]], [batch_size, beam_width])\n",
    "    sel_ids = tf.zeros([batch_size, beam_width, 0], dtype=ids.dtype)\n",
    "\n",
    "    mask = tf.ones([batch_size, beam_width], dtype=tf.float32)\n",
    "\n",
    "    for i in range(sequence_length):\n",
    "        with tf.variable_scope(name, reuse=True if i > 0 else None):\n",
    "            state, logits = update_fn(state, ids)\n",
    "            logits = tf.nn.log_softmax(logits)\n",
    "\n",
    "            sum_logprobs = (\n",
    "                tf.expand_dims(sel_sum_logprobs, axis=2) +\n",
    "                (logits * tf.expand_dims(mask, axis=2))\n",
    "            )\n",
    "\n",
    "            num_classes = logits.shape.as_list()[-1]\n",
    "\n",
    "            sel_sum_logprobs, indices = tf.nn.top_k(\n",
    "                tf.reshape(sum_logprobs, [batch_size, num_classes * beam_width]),\n",
    "                k=beam_width\n",
    "            )\n",
    "\n",
    "            ids = indices % num_classes\n",
    "\n",
    "            beam_ids = indices // num_classes\n",
    "\n",
    "            state = batch_gather(state, beam_ids)\n",
    "\n",
    "            sel_ids = tf.concat([batch_gather(sel_ids, beam_ids),\n",
    "                tf.expand_dims(ids, axis=2)], axis=2)\n",
    "\n",
    "            mask = (\n",
    "                batch_gather(mask, beam_ids) * tf.to_float(tf.not_equal(ids, end_token_id))\n",
    "            )\n",
    "\n",
    "    return sel_ids, sel_sum_logprobs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge <a name=\"merge\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def merge(tensors, units, activation=tf.nn.relu, name=None, **kwargs):\n",
    "    \"\"\"Merge features with broadcasting support.\n",
    "\n",
    "    This operation concatenates multiple features of varying length and applies\n",
    "    non-linear transformation to the outcome.\n",
    "\n",
    "    Example:\n",
    "        a = tf.zeros([m, 1, d1])\n",
    "        b = tf.zeros([1, n, d2])\n",
    "        c = merge([a, b], d3)  # shape of c would be [m, n, d3].\n",
    "\n",
    "    Args:\n",
    "        tensors: A list of tensor with the same rank.\n",
    "        units: Number of units in the projection function.\n",
    "    Returns:\n",
    "        result: A tensor being a (broadcasted) sum of prjected tensors.\n",
    "    \"\"\"\n",
    "    with tf.variable_scope(name, default_name=\"merge\"):\n",
    "        # Apply linear projection to input tensors.\n",
    "        projs = []\n",
    "        for i, tensor in enumerate(tensors):\n",
    "            proj = tf.layers.dense(\n",
    "                tensor, units, activation=None,\n",
    "                name=\"proj_%d\" % i,\n",
    "                **kwargs\n",
    "            )\n",
    "            projs.append(proj)\n",
    "\n",
    "        # Compute sum of tensors (this is where the broadcasting takes place).\n",
    "        result = projs.pop()\n",
    "        for proj in projs:\n",
    "            result = result + proj\n",
    "\n",
    "        # Apply nonlinearity.\n",
    "        if activation:\n",
    "            result = activation(result)\n",
    "            \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entropy <a name=\"entropy\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def softmax_entropy(logits, dim=-1):\n",
    "    \"\"\"Compute entropy over specified dimensions.\"\"\"\n",
    "    plogp = tf.nn.softmax(logits, dim) * tf.nn.log_softmax(logits, dim)\n",
    "    return -tf.reduce_sum(plogp, dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KL-Divergence <a name=\"kld\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gaussian_kl(q, p=(0., 0.)):\n",
    "    \"\"\"Compute KL divergence between two univariate Gaussian distributions.\n",
    "\n",
    "    To ensure numerical stability, this op uses mu, log(sigma^2) to represent\n",
    "    the distribution. If q is not provided, it's assumed to be unit Gaussian.\n",
    "    \n",
    "    Args:\n",
    "        q: A tuple (mu, log(sigma^2)) representing a multi-variatie Gaussian.\n",
    "        p: A tuple (mu, log(sigma^2)) representing a multi-variatie Gaussian.\n",
    "    Returns:\n",
    "        A tensor representing KL(q, p).\n",
    "    \"\"\"\n",
    "    mu1, log_sigma1_sq = q\n",
    "    mu2, log_sigma2_sq = p\n",
    "    kl_div = 0.5 * (\n",
    "        log_sigma2_sq - log_sigma1_sq +\n",
    "        tf.exp(log_sigma1_sq - log_sigma2_sq) +\n",
    "        tf.square(mu1 - mu2) / tf.exp(log_sigma2_sq) -\n",
    "        1\n",
    "    )\n",
    "    return tf.reduce_sum(kl_div, axis=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make parallel <a name=\"make_parallel\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_parallel(fn, num_gpus, **kwargs):\n",
    "    \"\"\"Parallelize given model on multiple gpu devices.\n",
    "\n",
    "    Args:\n",
    "        fn: Arbitrary function that takes a set of input tensors and outputs a\n",
    "            single tensor. First dimension of inputs and output tensor are assumed\n",
    "            to be batch dimension.\n",
    "        num_gpus: Number of GPU devices.\n",
    "        **kwargs: Keyword arguments to be passed to the model.\n",
    "    Returns:\n",
    "        A tensor corresponding to the model output.\n",
    "    \"\"\"\n",
    "    in_splits = {}\n",
    "    for k, v in kwargs.items():\n",
    "        in_splits[k] = tf.split(v, num_gpus)\n",
    "\n",
    "    out_split = []\n",
    "    for i in range(num_gpus):\n",
    "        with tf.device(tf.DeviceSpec(device_type=\"GPU\", device_index=i)):\n",
    "            with tf.variable_scope(tf.get_variable_scope(), reuse=i > 0):\n",
    "                out_split.append(fn(**{k : v[i] for k, v in in_splits.items()}))\n",
    "\n",
    "    return tf.concat(out_split, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Leaky relu <a name=\"leaky_relu\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def leaky_relu(tensor, alpha=0.1):\n",
    "    \"\"\"Computes the leaky rectified linear activation.\"\"\"\n",
    "    return tf.maximum(tensor, alpha * tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch normalization <a name=\"batch_norm\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def batch_normalization(tensor, training=False, epsilon=0.001, momentum=0.9, \n",
    "                        fused_batch_norm=False, name=None):\n",
    "    \"\"\"Performs batch normalization on a given 4-D tensor.\n",
    "\n",
    "    The features are assumed to be in NHWC format. Know, that you need to \n",
    "    run UPDATE_OPS in order for this function to perform correctly, e.g.:\n",
    "\n",
    "    with tf.control_dependencies(tf.get_collection(tf.GraphKeys.UPDATE_OPS)):\n",
    "        train_op = optimizer.minimize(loss)\n",
    "\n",
    "    Based on: https://arxiv.org/abs/1502.03167\n",
    "    \"\"\"\n",
    "    with tf.variable_scope(name, default_name=\"batch_normalization\"):\n",
    "        channels = tensor.shape.as_list()[-1]\n",
    "        axes = list(range(tensor.shape.ndims - 1))\n",
    "\n",
    "        beta = tf.get_variable(\n",
    "            'beta', channels, initializer=tf.zeros_initializer())\n",
    "        gamma = tf.get_variable(\n",
    "            'gamma', channels, initializer=tf.ones_initializer())\n",
    "\n",
    "        avg_mean = tf.get_variable(\n",
    "            'avg_mean', channels, initializer=tf.zeros_initializer(),\n",
    "            trainable=False)\n",
    "        avg_variance = tf.get_variable(\n",
    "            'avg_variance', channels, initializer=tf.ones_initializer(),\n",
    "            trainable=False)\n",
    "\n",
    "        if training:\n",
    "            if fused_batch_norm:\n",
    "                mean, variance = None, None\n",
    "            else:\n",
    "                mean, variance = tf.nn.moments(tensor, axes=axes)\n",
    "        else:\n",
    "            mean, variance = avg_mean, avg_variance\n",
    "\n",
    "        if fused_batch_norm:\n",
    "            tensor, mean, variance = tf.nn.fused_batch_norm(\n",
    "                tensor, scale=gamma, offset=beta, mean=mean, variance=variance, \n",
    "                epsilon=epsilon, is_training=training)\n",
    "        else:\n",
    "            tensor = tf.nn.batch_normalization(\n",
    "                tensor, mean, variance, beta, gamma, epsilon)\n",
    "\n",
    "        if training:\n",
    "            update_mean = tf.assign(\n",
    "                avg_mean, avg_mean * momentum + mean * (1.0 - momentum))\n",
    "            update_variance = tf.assign(\n",
    "                avg_variance, avg_variance * momentum + variance * (1.0 - momentum))\n",
    "\n",
    "            tf.add_to_collection(tf.GraphKeys.UPDATE_OPS, update_mean)\n",
    "            tf.add_to_collection(tf.GraphKeys.UPDATE_OPS, update_variance)\n",
    "\n",
    "    return tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Squeeze and excitation <a name=\"squeeze_excite\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def squeeze_and_excite(tensor, ratio=16, name=None):\n",
    "    \"\"\"Apply squeeze/excite on a given 4-D tensor.\n",
    "\n",
    "    Based on: https://arxiv.org/abs/1709.01507\n",
    "    \"\"\"\n",
    "    with tf.variable_scope(name, default_name=\"squeeze_and_excite\"):\n",
    "        original = tensor\n",
    "        units = tensor.shape.as_list()[-1]\n",
    "        tensor = tf.reduce_mean(tensor, [1, 2], keep_dims=True)\n",
    "        tensor = tf.layers.dense(tensor, units / ratio, use_bias=False)\n",
    "        tensor = tf.nn.relu(tensor)\n",
    "        tensor = tf.layers.dense(tensor, units, use_bias=False)\n",
    "        tensor = tf.nn.sigmoid(tensor)\n",
    "        tensor = original * tensor\n",
    "    return tensor"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
